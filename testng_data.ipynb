{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPb18pqYdCxa9Tkw5k+kPdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasW179/NLP-with-Python/blob/master/testng_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ra23Uh1FwCxk",
        "outputId": "c8545def-3921-445d-fc91-11ba10c0b1fe"
      },
      "source": [
        "!pip install texthero"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texthero\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/5a/a9d33b799fe53011de79d140ad6d86c440a2da1ae8a7b24e851ee2f8bde8/texthero-1.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.41.1)\n",
            "Collecting nltk>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.1.5)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.4.1)\n",
            "Collecting unidecode>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2.8.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.6.0->texthero) (4.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.0.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (54.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.4.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434675 sha256=4b020a10b2b7697d070870e7b05ba87f9a1a66b7427c3a8c8b9dee5ed8d6558e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk, unidecode, texthero\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5 texthero-1.0.9 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "oYBOHlTsv72S",
        "outputId": "019f40ab-cd58-4ca2-a1b5-8e2d1c90b6e1"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "#import transformers\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
        "from tensorflow.keras.layers import Dense\n",
        "tf.config.list_physical_devices(\"GPU\")\n",
        "tf.test.is_built_with_cuda()\n",
        "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "import re\n",
        "import texthero as hero\n",
        "from texthero import preprocessing\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()\n",
        "df = pd.read_csv('testing_data.csv')\n",
        "df.dropna(subset=['abstract'], inplace=True)\n",
        "\n",
        "\n",
        "empirical_journals = ['experimental economics', 'journal of applied economics',\n",
        "                      'empirical economics', 'american economic journal: applied economics',\n",
        "                      'applied economics perspectives and policy', \n",
        "                      'applied economics and finance', 'applied economics',\n",
        "                      ]\n",
        "\n",
        "theoretical_journals = ['journal of economics theory', 'american economic journal: microeconomics',\n",
        "                        'economic theory', 'theoretical economics']\n",
        "\n",
        "\n",
        "\n",
        "# import bs4 for cleaning\n",
        "'''\n",
        "from bs4 import BeautifulSoup\n",
        "def cleanText(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "    text = text.lower()\n",
        "    text = text.replace('x', '')\n",
        "    return text\n",
        "'''\n",
        "\n",
        "\n",
        "custom_pipe = [preprocessing.fillna,\n",
        "               #preprocessing.remove_brackets,\n",
        "               preprocessing.lowercase,\n",
        "               preprocessing.remove_whitespace,\n",
        "               preprocessing.remove_html_tags,\n",
        "               preprocessing.remove_diacritics,\n",
        "               ]\n",
        "\n",
        "df['abstract'] = hero.clean(df['abstract'], custom_pipe)\n",
        "df['abstract'] = [n.replace('{','') for n in df['abstract']]\n",
        "df['abstract'] = [n.replace('}','') for n in df['abstract']]\n",
        "df['abstract'] = [n.replace('(','') for n in df['abstract']]\n",
        "df['abstract'] = [n.replace(')','') for n in df['abstract']]\n",
        "\n",
        "\n",
        "\n",
        "econometrics_journals = ['journal of econometrics', 'econometric theory']\n",
        "\n",
        "\n",
        "df['journal'] = df.journal.str.lower()\n",
        "empirical = df.journal.isin(empirical_journals)\n",
        "theoretical = df.journal.isin(theoretical_journals)\n",
        "econometric = df.journal.isin(econometrics_journals)\n",
        "\n",
        "\n",
        "\n",
        "# Get training and test data, label target variables\n",
        "\n",
        "df['theoretical'] = df.journal.isin(theoretical_journals)\n",
        "df['empirical'] = df.journal.isin(empirical_journals)\n",
        "labels = df[['theoretical', 'empirical']]*1\n",
        "y = labels.idxmax(axis =1)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "train, test,y_train, y_test = train_test_split(df,y, test_size = 0.1, random_state =0)\n",
        "\n",
        "\n",
        "# Prepare for doc2vec\n",
        "\n",
        "tagged_documents = [TaggedDocument(t.split(' '), [i])\n",
        "                    for i, t in enumerate(df.abstract)]\n",
        "\n",
        "# Instantiate model\n",
        "\n",
        "model = Doc2Vec(vector_size = 100, window = 5, alpha = .025, \n",
        "                min_alpha = .00025, min_count =1, dm =1,\n",
        "                workers = cores)\n",
        "\n",
        "\n",
        "\n",
        "# Tag training & Test documents\n",
        "\n",
        "tagged_train = [TaggedDocument(t.split(' '), [i]) for i,t in\n",
        "                enumerate(train.abstract)]\n",
        "\n",
        "\n",
        "tagged_test = [TaggedDocument(t.split(' '),[i]) for i, t in enumerate(test.abstract)]\n",
        "\n",
        "# Get vocab and build doc2vec models on training set\n",
        "\n",
        "vocab = model.build_vocab(tagged_train)\n",
        "\n",
        "model.train(tagged_train, total_examples = model.corpus_count,epochs = 250)\n",
        "model.save('100_dim_doc2vec_model')\n",
        "\n",
        "\n",
        "\n",
        "doctags = model.docvecs.vectors_docs\n",
        "\n",
        "x_train = np.array([model.docvecs[i] for i in range(len(tagged_train))])\n",
        "x_test = np.array([model.infer_vector(tagged_test[i][0])\n",
        "                   for i in range(len(tagged_test))])\n",
        "\n",
        "kmeans = KMeans(init =  'k-means++',n_clusters = 2, max_iter = 700)\n",
        "kmeans.fit(x_train, y_train)\n",
        "klabels = kmeans.labels_\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "score = adjusted_rand_score(y_train, klabels)\n",
        "\n",
        "\n",
        "\n",
        "preds = kmeans.predict(x_test)\n",
        "score_pred = adjusted_rand_score(y_test, preds)\n",
        "accuracy_pred = accuracy_score(y_test, preds)\n",
        "\n",
        "\n",
        "# Try again with PCA\n",
        "pca = PCA(n_components= 2)\n",
        "pca_vecs = pca.fit_transform(x_train)\n",
        "kmeans.fit(pca_vecs)\n",
        "klabels = kmeans.labels_\n",
        "score2 = adjusted_rand_score(y_train, klabels)\n",
        "accuracy2 = accuracy_score(y_train, klabels)\n",
        "\n",
        "preds2 = kmeans.predict(pca.fit_transform(x_test))\n",
        "pscore_pred = adjusted_rand_score(y_test, preds2)\n",
        "accuracy_pred2 = accuracy_score(y_test, preds2)\n",
        "\n",
        "\n",
        "\n",
        "# Look at it with entire dataset\n",
        "\n",
        "model = Doc2Vec(vector_size = 100, window = 5, alpha = .025, \n",
        "                min_alpha = .00025, min_count =1, dm =1,\n",
        "                workers = cores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# do the same for 3oo dimensions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Doc2Vec(vector_size = 300, window = 5, alpha = .025, \n",
        "                min_alpha = 0.00025, min_count =1, dm =1, workers = cores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare with universal sentence encoder\n",
        "train, test = train_test_split(df, test_size = 0.01)\n",
        "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
        "embeddings = embed(df['abstract'])\n",
        "usage = np.array(embeddings).tolist()\n",
        "kmeans.fit(usage, y = y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-a5735dfe6eba>:15: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a5735dfe6eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtexthero\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtexthero\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'texthero'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}